---
title: "Simulating traits with GCTA"
author: "Frederick J. Boehm"
date: "`r Sys.Date()`"
output: html_document
---

We want to use GCTA software to simulate quantitative traits. Here is the
documentation: https://yanglab.westlake.edu.cn/software/gcta/#GWASSimulation

The default error distribution is ok. What is more concerning is the effect size distribution. let's use R to simulate the effect sizes. We can then save them as text files for use with GCTA, via the `--simu-causal-loci` flag. 





## Simulation effects

We first read in a bim file to get the names of the SNPs for Chr 22 from the UKB data.

```{r}
library(magrittr)
```


```{r}
bim_file <- "~/research/ukb-intervals/dat/plink_files/ukb/chr22.bim"
bim_tib <- vroom::vroom(bim_file, col_names = FALSE) %>%
  dplyr::mutate(snp_index = 1:nrow(.)) %>%
  dplyr::rename(chromosome = X1, snp_id = X2, pos_cm = X3, pos_bp = X4, a1 = X5, a2 = X6)
```


```{r}
m <- nrow(bim_tib)
hsq <- 0.1
p_causal <- 0.1
set.seed(2022-05-23)
bim_tib2 <- bim_tib %>%
  dplyr::mutate(causal = rbinom(n = m, size = 1, prob = p_causal)) %>%
  dplyr::mutate(effect = as.numeric(causal) * rnorm(n = m, mean = 0, sd = sqrt(hsq / (m * p_causal))))
# make causal.snplist for GCTA
bim_tib2 %>%
  dplyr::select(snp_id, effect) %>%
  vroom::vroom_write(file = "../dat/simulations-ding/snp_effects_Chr22_hsq0.1_pcausal0.1.txt",
                     col_names = FALSE)
#
hsq <- 0.2
bim_tib2 <- bim_tib %>%
  dplyr::mutate(causal = rbinom(n = m, size = 1, prob = p_causal)) %>%
  dplyr::mutate(effect = causal * rnorm(n = m, mean = 0, sd = sqrt(hsq / (m * p_causal))))
# make causal.snplist for GCTA
bim_tib2 %>%
  dplyr::select(snp_id, effect) %>%
  vroom::vroom_write(file = "../dat/simulations-ding/snp_effects_Chr22_hsq0.2_pcausal0.1.txt",
                     col_names = FALSE)
# 
hsq <- 0.5
bim_tib2 <- bim_tib %>%
  dplyr::mutate(causal = rbinom(n = m, size = 1, prob = p_causal)) %>%
  dplyr::mutate(effect = causal * rnorm(n = m, mean = 0, sd = sqrt(hsq / (m * p_causal))))
# make causal.snplist for GCTA
bim_tib2 %>%
  dplyr::select(snp_id, effect) %>%
  vroom::vroom_write(file = "../dat/simulations-ding/snp_effects_Chr22_hsq0.5_pcausal0.1.txt",
                     col_names = FALSE)
#
```

```{r}
hsq <- 0.2
p_causal <- c(0.001, 0.01, 1)
set.seed(2022-06-21)
for (i in 1:length(p_causal)){
  pc <- p_causal[i] 

  bim_tib2 <- bim_tib %>%
    dplyr::mutate(causal = rbinom(n = m, size = 1, prob = pc)) %>%
    dplyr::mutate(effect = causal * rnorm(n = m, mean = 0, sd = sqrt(hsq / (m * pc))))
  # make causal.snplist for GCTA
  outfn <- paste0("../dat/simulations-ding/snp_effects_Chr22_hsq", hsq, "_pcausal", pc, ".txt")
  bim_tib2 %>%
    dplyr::select(snp_id, effect) %>%
    vroom::vroom_write(file = outfn,
                       col_names = FALSE)
}
```

## Write a new fam file for use with GCTA

Sheng's fam files have too many columns for use with GCTA. We'll use it to write 
a fam file in standard format.

```{r}
fam_file <- "../dat/plink_files/ukb/chr22.fam"
vroom::vroom(fam_file, col_names = FALSE) %>%
  dplyr::select(1:6) %>%
  vroom::vroom_write(file = "../dat/simulations-ding/chr22.fam", col_names = FALSE)
```

Now, make symbolic links for bed and bim files

```{bash, eval = FALSE}
ln -s ../dat/plink_files/ukb/chr22.bed ../dat/simulations-ding/chr22.bed
ln -s ../dat/plink_files/ukb/chr22.bim ../dat/simulations-ding/chr22.bim
```

## GCTA to simulate traits

Now, let's run GCTA to simulate traits.

```{bash, eval = TRUE}

gcta64 --bfile ../dat/simulations-ding/chr22 --simu-qt --simu-causal-loci  ../dat/simulations-ding/snp_effects_Chr22_hsq0.2_pcausal0.1.txt --simu-hsq 0.2 --simu-rep 10 --out ../dat/simulations-ding/sim_traits/sims_Chr22_hsq0.2_pcausal0.1

gcta64 --bfile ../dat/simulations-ding/chr22 --simu-qt --simu-causal-loci ../dat/simulations-ding/snp_effects_Chr22_hsq0.1_pcausal0.1.txt --simu-hsq 0.1 --simu-rep 10 --out ../dat/simulations/sim_traits/sims_Chr22_hsq0.1_pcausal0.1

gcta64 --bfile ../dat/simulations-ding/chr22 --simu-qt --simu-causal-loci ../dat/simulations-ding/snp_effects_Chr22_hsq0.5_pcausal0.1.txt --simu-hsq 0.5 --simu-rep 10 --out ../dat/simulations/sim_traits/sims_Chr22_hsq0.5_pcausal0.1
```

```{bash}
for pcausal in 0.001 0.01 1; do
  gcta64 --bfile ../dat/simulations-ding/chr22 --simu-qt --simu-causal-loci   ../dat/simulations-ding/snp_effects_Chr22_hsq0.2_pcausal${pcausal}.txt --simu-hsq 0.2 --simu-rep 10 --out ../dat/simulations-ding/sim_traits/sims_Chr22_hsq0.2_pcausal${pcausal}
  
done  
  
```


## Divide subjects

We next need to divide the subjects into training, validation and test subjects.

We'll first read in the fam file to get the sample ids, then use R functions to sample from the ids.

```{r}
set.seed(2022-06-01)
ids <- vroom::vroom(file = "../dat/simulations-ding/chr22.fam", col_names = FALSE)
pre_val_ids <- sample(x = ids$X1, size = 37129, replace = FALSE) 
# split pre_val_ids into two - one validation set and a "verification" set of, say, 1000.
# verification set is used in the cv, as the X_{n+1}, while validation is used in the tuning of parameters for DBSLMM
verif_ids <- sample(pre_val_ids, size = 1000, replace = FALSE)
val_ids <- setdiff(pre_val_ids, verif_ids)
# Write to files
val_ids %>%
  tibble::tibble(.name_repair = "universal") %>%
  dplyr::rename(X1 = 1) %>%
  dplyr::arrange(X1) %>%
  dplyr::mutate(X2 = X1) %>%
  vroom::vroom_write(file = "../dat/simulations-ding/validation-ids.txt", col_names = FALSE)
verif_ids %>%
  tibble::tibble(.name_repair = "universal") %>%
  dplyr::rename(X1 = 1) %>%
  dplyr::arrange(X1) %>%
  dplyr::mutate(X2 = X1) %>%
  vroom::vroom_write(file = "../dat/simulations-ding/verification-ids.txt", col_names = FALSE)

```


```{r}
test_and_training_ids <- setdiff(ids$X1, val_ids)
# set up for 5-fold cv with the remaining 300,000 subjects
set.seed(2022-06-10)
ids_shuffled <- sample(x = test_and_training_ids, size = length(test_and_training_ids))
folds <- cut(seq(1,length(ids_shuffled)),breaks=5,labels=FALSE)
# https://stats.stackexchange.com/questions/61090/how-to-split-a-data-set-to-do-10-fold-cross-validation
for (i in 1:5){
  ids_tib <- tibble::tibble(ids_shuffled, folds) %>%
    dplyr::arrange(ids_shuffled)
  ids_tib %>%
    dplyr::filter(folds == i) %>%
    dplyr::mutate(X2 = ids_shuffled) %>%
    dplyr::select(- folds) %>%
    vroom::vroom_write(file = paste0("../dat/simulations-ding/test-ids-fold", i, ".txt"), col_names = FALSE)
  ids_tib %>%
    dplyr::filter(folds != i) %>%
    dplyr::mutate(X2 = ids_shuffled) %>%
    dplyr::select(- folds) %>%
    vroom::vroom_write(file = paste0("../dat/simulations-ding/training-ids-fold", i, ".txt"), col_names = FALSE)
  
}
```

## Quantile normalization of the traits

https://davetang.org/muse/2014/07/07/quantile-normalisation-in-r/

https://en.wikipedia.org/wiki/Quantile_normalization

http://jtleek.com/genstats/inst/doc/02_05_normalization.html

We next need to quantile normalize the simulated traits. We can do this with the 
function `preprocessCore::normalize.quantiles` from the Bioconductor R package
`preprocessCore`. 

```{r}
settings <- tibble::tibble(hsq = c(0.2, 0.2, 0.2, 0.1, 0.5), p_causal = c(0.001, 0.01, 1, 0.1, 0.1))

# read a file containing simulated traits
for (row in 1:nrow(settings)){
  hsq <- settings$hsq[row]
  p_causal <- settings$p_causal[row]
  trait_file <- paste0("../dat/simulations-ding/sim_traits/sims_Chr22_hsq", hsq, "_pcausal", p_causal, ".phen")
  trait_tib <- vroom::vroom(trait_file, col_names = FALSE) %>%
    dplyr::select(-13) # drop the last column which contains only NAs
  
  # read files to get ids for training, test, and validation sets
  val_ids <- vroom::vroom(file = "../dat/simulations-ding/validation-ids.txt", col_names = TRUE)
  verif_ids <- vroom::vroom(file = "../dat/simulations-ding/verification-ids.txt", col_names = TRUE)
  test_ids <- list()
  training_ids <- list()
  #
  for (fold in 1:5){
    test_ids[[fold]] <- vroom::vroom(file = paste0("../dat/simulations-ding/test-ids-fold", fold, ".txt"), col_names = FALSE)
    training_ids[[fold]] <- vroom::vroom(file = paste0("../dat/simulations-ding/training-ids-fold", fold, ".txt"), col_names = FALSE)
  
  # use training set only to determine the distribution for quantile normalization
  # then, use the inferred distribution to quantile normalize, separately, the training and the validation and the verification and the test sets.
    trait_tib_training <- trait_tib %>%
      dplyr::filter(X1 %in% training_ids[[fold]]$X1)
    trait_tib_test <- trait_tib %>%
      dplyr::filter(X1 %in% test_ids[[fold]]$X1)
    trait_tib_validation <- trait_tib %>%
      dplyr::filter(X1 %in% val_ids$X1)
    trait_tib_verification <- trait_tib %>%
      dplyr::filter(X1 %in% verif_ids$X1)
    
    
    training_qn <- preprocessCore::normalize.quantiles(as.matrix(trait_tib_training[, 3:12]))
    target_test <- preprocessCore::normalize.quantiles.determine.target(x = training_qn, target.length = nrow(trait_tib_test))
    test_qn <- preprocessCore::normalize.quantiles.use.target(x = as.matrix(trait_tib_test[, 3:12]), target = target_test)
    target_validation <- preprocessCore::normalize.quantiles.determine.target(x = training_qn, target.length = nrow(trait_tib_validation))
    validation_qn <- preprocessCore::normalize.quantiles.use.target(x = as.matrix(trait_tib_validation[, 3:12]), target = target_validation)
    target_verification <- preprocessCore::normalize.quantiles.determine.target(x = training_qn, target.length = nrow(trait_tib_verification))
    verification_qn <- preprocessCore::normalize.quantiles.use.target(x = as.matrix(trait_tib_verification[, 3:12]), target = target_verification)

    ######
    training_qn_tib <- training_ids[[fold]] %>%
      dplyr::bind_cols(tibble::as_tibble(training_qn))
    test_qn_tib <- test_ids[[fold]] %>%
      dplyr::bind_cols(tibble::as_tibble(test_qn))
    validation_qn_tib <- val_ids %>%
      dplyr::bind_cols(tibble::as_tibble(validation_qn))
    verification_qn_tib <- verif_ids %>%
      dplyr::bind_cols(tibble::as_tibble(verification_qn))
    
    qn_tib_pre <- training_qn_tib %>%
      dplyr::bind_rows(test_qn_tib) %>%
      dplyr::bind_rows(validation_qn_tib) %>%
      dplyr::bind_rows(verification_qn_tib) %>%
      dplyr::arrange(X1) #%>%
    qn_tib <- qn_tib_pre %>%
      dplyr::left_join(ids, by = c("X1", "X2")) %>%
      dplyr::select(X1, X2, X3, X4, X5, V1:V10)
#    vroom::vroom_write(x = qn_tib, file = paste0("../dat/simulations-ding/sim_traits/sims_Chr22_hsq", hsq, "_pcausal", p_causal, ".fam"), col_names = FALSE)
} # end loop over folds

    false_to_na <- function(vec){
      vec[!vec] <- NA
      return(vec)
    }
  # make binary indicators of membership in training set
  tr_indic <- tibble::tibble(train1 = qn_tib$X1 %in% training_ids[[1]]$X1,
                             train2 = qn_tib$X1 %in% training_ids[[2]]$X1,
                             train3 = qn_tib$X1 %in% training_ids[[3]]$X1,
                             train4 = qn_tib$X1 %in% training_ids[[4]]$X1,
                             train5 = qn_tib$X1 %in% training_ids[[5]]$X1
                             ) %>%
    dplyr::mutate(fold1_na = false_to_na(train1),
                  fold2_na = false_to_na(train2),
                  fold3_na = false_to_na(train3),
                  fold4_na = false_to_na(train4),
                  fold5_na = false_to_na(train5)
                  )
  qn_tib2 <- qn_tib %>%
    dplyr::mutate(tr1_fold1 = V1 * tr_indic$fold1_na,
                  tr1_fold2 = V1 * tr_indic$fold2_na,
                  tr1_fold3 = V1 * tr_indic$fold3_na,
                  tr1_fold4 = V1 * tr_indic$fold4_na,
                  tr1_fold5 = V1 * tr_indic$fold5_na,
                  tr2_fold1 = V2 * tr_indic$fold1_na,
                  tr2_fold2 = V2 * tr_indic$fold2_na,
                  tr2_fold3 = V2 * tr_indic$fold3_na,
                  tr2_fold4 = V2 * tr_indic$fold4_na,
                  tr2_fold5 = V2 * tr_indic$fold5_na,
                  tr3_fold1 = V3 * tr_indic$fold1_na,
                  tr3_fold2 = V3 * tr_indic$fold2_na,
                  tr3_fold3 = V3 * tr_indic$fold3_na,
                  tr3_fold4 = V3 * tr_indic$fold4_na,
                  tr3_fold5 = V3 * tr_indic$fold5_na,
                  tr4_fold1 = V4 * tr_indic$fold1_na,
                  tr4_fold2 = V4 * tr_indic$fold2_na,
                  tr4_fold3 = V4 * tr_indic$fold3_na,
                  tr4_fold4 = V4 * tr_indic$fold4_na,
                  tr4_fold5 = V4 * tr_indic$fold5_na,
                  tr5_fold1 = V5 * tr_indic$fold1_na,
                  tr5_fold2 = V5 * tr_indic$fold2_na,
                  tr5_fold3 = V5 * tr_indic$fold3_na,
                  tr5_fold4 = V5 * tr_indic$fold4_na,
                  tr5_fold5 = V5 * tr_indic$fold5_na,
                  tr6_fold1 = V6 * tr_indic$fold1_na,
                  tr6_fold2 = V6 * tr_indic$fold2_na,
                  tr6_fold3 = V6 * tr_indic$fold3_na,
                  tr6_fold4 = V6 * tr_indic$fold4_na,
                  tr6_fold5 = V6 * tr_indic$fold5_na,
                  tr7_fold1 = V7 * tr_indic$fold1_na,
                  tr7_fold2 = V7 * tr_indic$fold2_na,
                  tr7_fold3 = V7 * tr_indic$fold3_na,
                  tr7_fold4 = V7 * tr_indic$fold4_na,
                  tr7_fold5 = V7 * tr_indic$fold5_na,
                  tr8_fold1 = V8 * tr_indic$fold1_na,
                  tr8_fold2 = V8 * tr_indic$fold2_na,
                  tr8_fold3 = V8 * tr_indic$fold3_na,
                  tr8_fold4 = V8 * tr_indic$fold4_na,
                  tr8_fold5 = V8 * tr_indic$fold5_na,
                  tr9_fold1 = V9 * tr_indic$fold1_na,
                  tr9_fold2 = V9 * tr_indic$fold2_na,
                  tr9_fold3 = V9 * tr_indic$fold3_na,
                  tr9_fold4 = V9 * tr_indic$fold4_na,
                  tr9_fold5 = V9 * tr_indic$fold5_na,
                  tr10_fold1 = V10 * tr_indic$fold1_na,
                  tr10_fold2 = V10 * tr_indic$fold2_na,
                  tr10_fold3 = V10 * tr_indic$fold3_na,
                  tr10_fold4 = V10 * tr_indic$fold4_na,
                  tr10_fold5 = V10 * tr_indic$fold5_na,
                  ) %>%
    dplyr::select(-V1, - V2, - V3, -V4, -V5, -V6, -V7, -V8, -V9, -V10) 
  qn_tib3 <- qn_tib %>%
    mutate_product(trait_num = 1:10, fold_num = 1:5) %>%
    dplyr::select(-V1, - V2, - V3, -V4, -V5, -V6, -V7, -V8, -V9, -V10) 
  identical(qn_tib2, qn_tib3)
  colnames(qn_tib2) == colnames(qn_tib3)
  qn_tib2$X1 %>% duplicated %>% sum()
  #####
  qn_tib2 %>%
    vroom::vroom_write(file = paste0("../dat/simulations-ding/sim_traits/sims_Chr22_hsq", hsq, "_pcausal", p_causal, "-NAs.fam"), col_names = FALSE)
}  
  
```

```{r}
# make the pheno files for verification set
qt_verif <- qn_tib %>%
  dplyr::arrange(X1) %>%
  dplyr::filter(X1 %in% verif_ids$X1)
for (col_num in 6:15){
  qt_verif %>%
    dplyr::select(dplyr::all_of(col_num)) %>%
    vroom::vroom_write(here::here("dat", "simulations-ding", "verification", paste0("pheno", col_num - 5, ".txt")), col_names = FALSE)
}
```

```{r}
# write qn traits for all subjects in a single fam file
qn_tib %>%
  vroom::vroom_write(here::here("dat", "simulations-ding", "sim_traits", "fam_all_subjects", "qn_traits_all_subjects_hsq0.2_pcausal0.001.fam"), col_names = FALSE)
```

The catch is that we want to use only the training set for quantile normalization. We then need to use those quantiles - from the training set - to normalize the test set and validation set.



```{r, eval = FALSE}
BiocManager::install(c("Biobase","preprocessCore"))
preprocessCore::normalize.quantiles
preprocessCore::normalize.quantiles.use.target
preprocessCore::normalize.quantiles.determine.target
```



```{r}
# check to see that qn traits have similar distributions
quantile(test_qn[, 1])
quantile(validation_qn[, 1])
quantile(test_qn[, 2])
quantile(training_qn[, 1])
hist(training_qn[, 1])
hist(training_qn[, 10])
qqplot(training_qn[, 10], test_qn[, 10])
# put all 3 sets back together
```

## Write fam files

We need to consider what we want the final fam file to look like. Should it have 
every subject's quantile-normalized trait value? Do we want a single fam file with all ten traits (one trait per column)? This seems reasonable - just need to figure out how to use ldsc with 
a fam file that has multiple traits

## Symbolic links for plink files

We're now working with 5 settings of the simulation parameters. 
Each setting has 10 traits * 5 folds in a single fam file. 
We need to create, for each of 5 settings, a directory with the plink files. 

```{bash}
let k=0

for pc in 0.001 0.01 1 0.1 0.1; do
  let k=${k}+1
    hsq=0.2
    if [ ${k} -eq 4 ]; then
      hsq=0.1
    fi
    if [ ${k} -eq 5 ]; then
      hsq=0.5
    fi
    newdir=~/research/ukb-intervals/dat/simulations-ding/gemma_hsq${hsq}_pcausal${pc}
    mkdir ${newdir}
    for chr in `seq 1 22`; do
      ln -s /net/mulan/disk2/yasheng/predictionProject/plink_file/ukb/chr${chr}.bed ${newdir}/chr${chr}.bed
      ln -s /net/mulan/disk2/yasheng/predictionProject/plink_file/ukb/chr${chr}.bim ${newdir}/chr${chr}.bim
      ln -s ../sim_traits/sims_Chr22_hsq${hsq}_pcausal${pc}-NAs.fam ${newdir}/chr${chr}.fam
    done
done
    
```
## ldsc to estimate trait heritability in the validation set

After quantile normalization, we need to use `ldsc` to estimate the traits' heritabilities. 
Note that $\hat h^2$ is an input to DBSLMM, so this step is needed before running DBSLMM. 

`--keep` flag to ldsc to use only a subset of the subjects. Does this flag behave like the `--keep` flag in plink??

First, we make a fam file with all training plus test set subjects (ie, excluding the validation set subjects). We need to prepare the file 
so that test set subjects, for a given fold, have missing values for the trait. The first five trait columns will all be simulated trait 1, then the next five are simulated trait 2, etc.


```{bash}
mkdir ../dat/simulations-ding/gemma 
cd ../dat/simulations-ding/gemma
for chr in `seq 1 22`
do
ln -s /net/mulan/disk2/yasheng/predictionProject/plink_file/ukb/chr${chr}.bed 
ln -s /net/mulan/disk2/yasheng/predictionProject/plink_file/ukb/chr${chr}.bim
ln -s ../sim_traits/sims_Chr22_hsq0.2_pcausal0.1-NAs.fam chr${chr}.fam
done


```

## DBSLMM for the simulated traits


We need to make the "validation pheno" files for use with DBSLMM. These consist of a single column in a text file. The column is the qn trait values for a single trait. Each row is one subject in the validation set, ordered by id number, from smallest (at the top of the file) to largest (at the bottom).

```{r}
val_ids <- vroom::vroom(file = "../dat/simulations-ding/validation-ids.txt", col_names = TRUE)


qn_tib_val <- vroom::vroom(file = "../dat/simulations-ding/sim_traits/sims_Chr22_hsq0.2_pcausal0.1.fam", col_names = FALSE) %>%
  dplyr::filter(X1 %in% val_ids$X1)
#X6 through X15 are the ten traits
for (col in 6:15){
  fn <- paste0("../dat/simulations-ding/validation/pheno", (col - 5), ".txt")
  qn_tib_val %>%
    dplyr::select(dplyr::all_of(col)) %>%
    vroom::vroom_write(file = fn, col_names = FALSE)
}
qn_tib_verif <- vroom::vroom(file = "../dat/simulations-ding/sim_traits/sims_Chr22_hsq0.2_pcausal0.1.fam", col_names = FALSE) %>%
  dplyr::filter(X1 %in% verif_ids$X1)
#X6 through X15 are the ten traits
for (col in 6:15){
  fn <- paste0("../dat/simulations-ding/verification/pheno", (col - 5), ".txt")
  qn_tib_verif %>%
    dplyr::select(dplyr::all_of(col)) %>%
    vroom::vroom_write(file = fn, col_names = FALSE)
}
```


